{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"n: %s \" %len(df))\n",
    "print(\"n_columns: %s \" %df.shape[1])\n",
    "\n",
    "print(\"n_unique: %s \" %df['id'].nunique())\n",
    "print(\"n_dups: %s \" %(len(df) - df['id']nunique()))\n",
    "\n",
    "# Set id as row index\n",
    "df = df.set_index('id')\n",
    "\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "print('\\nMissing values:', df['var1'].isna().sum())\n",
    "\n",
    "# Check levels for categorical variable \n",
    "\n",
    "print('\\n', df.groupby('var_cat').count()['id'])\n",
    "\n",
    "# Check missing values for each column \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new features\n",
    "\n",
    "# Transform categorical variables to dummies / one-hot encoding \n",
    "\n",
    "df = pd.get_dummies(df, prefix = ['dummy1', 'dummy2', 'dummy3'], \n",
    "               columns = ['var1', 'var2', 'var3'], dtype = int) \n",
    "\n",
    "# Subset variables \n",
    "\n",
    "df_subset = df.drop(columns=[])\n",
    "df_subset.dtypes\n",
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction - PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise selected fields to reduce feature space size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise select fields in agents df \n",
    "\n",
    "cols_to_norm = ['var1', 'var2', 'var3'] # numeric variables\n",
    "\n",
    "df[cols_to_norm] = df[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "df['var1'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=25, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "    \n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise number of principle components\n",
    "\n",
    "exp_var = pca.explained_variance_ratio_\n",
    "exp_var_cum =np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "print(exp_var_cum)\n",
    "plt.plot(exp_var_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA using optimal components \n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "pca.fit(df)\n",
    "df_reduced = pca.transform(df)\n",
    "\n",
    "df_reduced = pd.DataFrame(df_reduced, index = df.index)\n",
    "df_reduced.columns = ['PC1','PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10']\n",
    "df_reduced.shape\n",
    "df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PCA output to pickle file in order to recreate clustering output \n",
    "# -- only save once the clustering has been optimised \n",
    "\n",
    "# Load pca feature df to recreate clustering output\n",
    "df_reduced = pd.read_pickle('df_pca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Visualise agents in 3d space using the first 3 dimensions\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(df['PC1'], df['PC2'], df['PC3'])\n",
    "plt.savefig('3d.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pca feature df to recreate clustering output\n",
    "df_reduced = pd.read_pickle('df_pca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inertia (within cluster sum-of-squares) - how internally coherent clusters are \n",
    "\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,36) \n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df_reduced)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=20, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=100, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit optimal number of cluster to the data \n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state = 100)\n",
    "clusters = kmeans.fit(df_reduced)\n",
    "\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9883.184703826757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.labels_\n",
    "clusters.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33983, 21)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced['cluster'] = pd.Series(clusters.labels_, index=df_reduced.index)\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reduced.groupby('cluster').count()[['PC1']])\n",
    "agents_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate clusters and assign labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise clusters \n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(df_reduced['PC1'], df_reduced['PC2'], df_reduced['PC3'], c= df_reduced['cluster'])\n",
    "plt.savefig('3d.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33983, 222)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join new clusters to the original agents table \n",
    "\n",
    "agent_review = agents.join(agents_reduced['cluster'], on = 'id', how = 'left')\n",
    "agent_review['Cluster'] = agent_review['cluster'] + 1 \n",
    "agent_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate clusters \n",
    "\n",
    "counts = agent_review.groupby('Cluster').count()['var1']\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = sns.barplot(x = counts.index, y= counts)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10), textcoords='offset points'\n",
    "               )\n",
    "    \n",
    "ax.set_ylabel('')    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of cluster properties \n",
    "\n",
    "# Subset numerical agent fields and summarise for each cluster \n",
    "\n",
    "df_numeric = df_review[[\n",
    "                'var1', 'var2','var3'\n",
    "                ,'Cluster']].groupby('Cluster').sum()\n",
    "\n",
    "# Plot heatmap\n",
    "\n",
    "# normalise summary statistics for each cluster to make the comparable across variables\n",
    "cols_to_norm = df_numeric.columns\n",
    "df_numeric[cols_to_norm] = df_numeric[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df_numeric.transpose(), cmap=\"PuBu\", linewidths=0.5)#.set_title('Cluster Summary Heatmap', fontsize = 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "df_review[(df_review['Cluster'] == 13) & (df_review['var1'] == 1) ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pca feature df to recreate clustering output\n",
    "df_reduced = pd.read_pickle('df_pca.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering \n",
    "\n",
    "#Â Reset \n",
    "#del dbscan, clusters\n",
    "\n",
    "dbscan = DBSCAN(eps= 0.85, min_samples = 10) #Â eps=1, min_sample=10\n",
    "# eps has been optimised by the number and size of clusters that are generated. \n",
    "\n",
    "clusters = dbscan.fit(df_reduced)\n",
    "\n",
    "#clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join cluster labels to agents list \n",
    "df_reduced['cluster'] = pd.Series(clusters.labels_, index=df_reduced.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reduced.groupby('cluster').count()[['PC1']])\n",
    "#df_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join new clusters to the original agents table \n",
    "\n",
    "df_review = agents.join(df_reduced['cluster'], on = 'id', how = 'left')\n",
    "df_review['Cluster'] = df_review['cluster'] + 1 \n",
    "df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate clusters \n",
    "\n",
    "counts = df_review.groupby('Cluster').count()['var1']\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "ax = sns.barplot(x = counts.index, y= counts)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=11, color='gray', xytext=(0, 10), textcoords='offset points'\n",
    "               )\n",
    "    \n",
    "ax.set_ylabel('')    \n",
    "\n",
    "plt.savefig('cluster_summary.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Output cluster assignment\n",
    "\n",
    "df_review['Cluster'].to_csv('cluster_output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
